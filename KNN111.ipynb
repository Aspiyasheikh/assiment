{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfddaf0f-f569-4571-ba83-8455bc91fb44",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea36da14-74e7-4d95-acb6-334614521916",
   "metadata": {},
   "source": [
    "Ans : K-Nearest Neighbors (KNN) - In Short\n",
    "KNN is a supervised machine learning algorithm used for classification and regression. It finds the K closest data points (neighbors) using distance metrics like Euclidean distance and predicts based on majority voting (classification) or averaging (regression).\n",
    "\n",
    "No training phase → Just stores the dataset.\n",
    "\n",
    "Finds nearest neighbors → Compares with existing data points.\n",
    "\n",
    "Simple but slow → Works well for small datasets but is computationally expensive for large ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e292fc-6c76-4c4e-af50-f975896aff7f",
   "metadata": {},
   "source": [
    "Q2. How  do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d246c4-d5a6-496f-a3ad-e1813406c6d8",
   "metadata": {},
   "source": [
    "Ans:Choosing the right K value in KNN is crucial for model performance:\n",
    "\n",
    "Small K (e.g., 1-5): The model becomes too sensitive to noise, meaning it might misclassify due to outliers, leading to overfitting (memorizing the data instead of generalizing).\n",
    "\n",
    "Large K: The model considers more neighbors, which smooths the decision boundary but may lead to underfitting, meaning it loses important details.\n",
    "\n",
    "Optimal K Selection: A common approach is using cross-validation to test different values and find the one that minimizes error. A frequently used starting point is √N (square root of the dataset size).\n",
    "\n",
    "Odd vs. Even K: Choosing an odd K helps in classification problems to avoid ties between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc815d5c-ea7e-4680-a577-640db66f49bb",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc0781-c34b-4ab1-a6f1-064134d6cb33",
   "metadata": {},
   "source": [
    "Ans: The KNN Classifier and KNN Regressor both use the K-Nearest Neighbors algorithm but serve different purposes:\n",
    "\n",
    "KNN Classifier is used for categorical outputs (classification tasks). It looks at the K nearest neighbors and assigns the most common class among them as the final prediction. Example: Determining if a fruit is an apple or orange.\n",
    "\n",
    "KNN Regressor is used for continuous outputs (regression tasks). Instead of assigning a class, it calculates the average (or weighted average) of the neighbors' values to make a prediction. Example: Predicting the price of a house based on nearby houses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb3e57-63e1-46d7-99dc-dd6deed56030",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf4cec4-1f9b-4247-aaea-5198e253b1ed",
   "metadata": {},
   "source": [
    "Ans: The performance of KNN can be measured using different metrics depending on whether it's used for classification or regression:\n",
    "\n",
    "For KNN Classification\n",
    "Accuracy – Percentage of correct predictions.\n",
    "\n",
    "Precision, Recall, and F1-score – Useful for imbalanced datasets.\n",
    "\n",
    "Confusion Matrix – Shows true positives, false positives, etc.\n",
    "\n",
    "ROC Curve & AUC – Measures model performance at different thresholds.\n",
    "\n",
    "For KNN Regression\n",
    "Mean Squared Error (MSE) – Measures average squared difference between actual and predicted values.\n",
    "\n",
    "Mean Absolute Error (MAE) – Measures average absolute difference between actual and predicted values.\n",
    "\n",
    "R² Score (Coefficient of Determination) – Measures how well predictions fit the actual data (closer to 1 is better)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb116d1-120b-4327-9e25-47e49a4c1818",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f101bd-b528-4885-ac90-adc2bd86f82f",
   "metadata": {},
   "source": [
    "Ans:The curse of dimensionality in KNN refers to the problem where the performance of the algorithm decreases as the number of features (dimensions) increases.\n",
    "\n",
    "Why does it happen?\n",
    "Increased Sparsity – In high dimensions, data points become more spread out, making it harder to find close neighbors.\n",
    "\n",
    "Distance Becomes Less Meaningful – The difference between the nearest and farthest points reduces, making distance-based methods like KNN less effective.\n",
    "\n",
    "Computational Complexity – More dimensions increase the time required to compute distances and find neighbors.\n",
    "\n",
    "How to handle it?\n",
    "Feature Selection – Keep only the most relevant features.\n",
    "\n",
    "Dimensionality Reduction – Use PCA (Principal Component Analysis) or t-SNE to reduce dimensions.\n",
    "\n",
    "Scaling & Normalization – Standardizing data helps in maintaining meaningful distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d28abb-9d58-4ce2-a603-a970cbffe7fe",
   "metadata": {},
   "source": [
    "Q6.How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced34bc4-e17e-4919-b1b8-1f18848e67ae",
   "metadata": {},
   "source": [
    "Ans: Handling missing values in KNN is important to ensure accurate predictions. Here are some common techniques:\n",
    "\n",
    "1. Remove Rows with Missing Values\n",
    "If only a few rows have missing values, you can simply drop them.\n",
    "\n",
    "Suitable when the dataset is large and missing data is minimal.\n",
    "\n",
    "2. Impute Missing Values\n",
    "Mean/Median Imputation – Replace missing values with the column’s mean or median (works well for numerical data).\n",
    "\n",
    "Mode Imputation – Use the most frequent value for categorical features.\n",
    "\n",
    "3. KNN-Based Imputation\n",
    "Use KNN Imputer, which replaces missing values using the average of the nearest K neighbors based on feature similarity.\n",
    "\n",
    "More accurate than simple mean/median imputation.\n",
    "\n",
    "4. Predict Missing Values\n",
    "Use a separate machine learning model (like Linear Regression or Decision Trees) to predict and fill missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590b7e75-036a-4feb-84e5-e93a2eaf76d0",
   "metadata": {},
   "source": [
    "Q7. compare and contrast the performance of the KNN classifier and regressor.Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8982093a-1892-4248-a7bc-bb4506e3431b",
   "metadata": {},
   "source": [
    "Ans:The KNN classifier and KNN regressor serve different purposes based on the type of problem. The KNN classifier is used for classification tasks where the target variable is categorical, such as spam detection or disease prediction. It determines the class of a data point by majority voting among its k-nearest neighbors. In contrast, the KNN regressor is used for regression tasks where the target variable is continuous, such as predicting house prices or temperature. Instead of voting, it takes the average (or weighted average) of the k-nearest neighbors' values to make predictions. While both methods rely on distance-based similarity, the classifier's performance is typically evaluated using accuracy, precision, recall, or F1-score, whereas the regressor is assessed using RMSE, MAE, or R² score. The classifier can struggle with imbalanced datasets, while the regressor is more sensitive to noisy data and outliers. KNN is computationally expensive for large datasets, but it performs well on small datasets. Choosing between them depends on whether the problem requires class label predictions (classifier) or numerical value predictions (regressor).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77def20f-6ee1-4848-8ad0-6fbef0df7224",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ed383-634c-490f-8ad2-35c8c34a79bd",
   "metadata": {},
   "source": [
    "Ans:Strengths and Weaknesses of KNN\n",
    "Strengths:\n",
    "Simple, easy to implement, and non-parametric (no assumptions about data).\n",
    "\n",
    "Works well for both classification and regression, especially on small datasets.\n",
    "\n",
    "Effective when data is well-structured and noise-free.\n",
    "\n",
    "Weaknesses:\n",
    "Computationally expensive for large datasets due to distance calculations.\n",
    "\n",
    "Sensitive to noise, outliers, and imbalanced data.\n",
    "\n",
    "Struggles with high-dimensional data (curse of dimensionality).\n",
    "\n",
    "Solutions:\n",
    "Use KD-Trees, Ball Trees, or Approximate Nearest Neighbors (ANN) for faster search.\n",
    "\n",
    "Apply feature scaling, outlier removal, and weighted KNN to improve accuracy.\n",
    "\n",
    "Reduce dimensionality using PCA or feature selection to mitigate the curse of dimensionality.\n",
    "\n",
    "Handle imbalanced data with SMOTE or weighted KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115fede-3ac5-44df-8600-85d22541351f",
   "metadata": {},
   "source": [
    "Q9.What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7b906e-3ec9-4810-8591-ba0d1310fc88",
   "metadata": {},
   "source": [
    "Ans:Difference Between Euclidean Distance and Manhattan Distance in KNN\n",
    "\n",
    "Path Type:\n",
    "\n",
    "Euclidean: Measures the shortest straight-line distance.\n",
    "\n",
    "Manhattan: Measures distance by grid-like paths.\n",
    "\n",
    "Best for:\n",
    "\n",
    "Euclidean: Works well for continuous data with smooth distributions.\n",
    "\n",
    "Manhattan: Suitable for grid-based data or independent features.\n",
    "\n",
    "Computation Complexity:\n",
    "\n",
    "Euclidean: More complex due to the square root operation.\n",
    "\n",
    "Manhattan: Simpler and faster as it only involves absolute differences.\n",
    "\n",
    "Effect of High Dimensions:\n",
    "\n",
    "Euclidean: Affected by the curse of dimensionality (becomes less reliable in high dimensions).\n",
    "\n",
    "Manhattan: Less affected compared to Euclidean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3877a70b-a5b2-41f7-864e-80e8223ab8c7",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scalling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda68c3-f753-40ab-89c2-7727f6e1c714",
   "metadata": {},
   "source": [
    "Ans:Role of Feature Scaling in KNN\n",
    "Ensures Fair Distance Calculation:\n",
    "\n",
    "KNN relies on distance metrics (e.g., Euclidean, Manhattan).\n",
    "\n",
    "Features with larger ranges dominate those with smaller ranges.\n",
    "\n",
    "Improves Model Accuracy:\n",
    "\n",
    "Prevents bias towards features with higher numerical values.\n",
    "\n",
    "Ensures all features contribute equally to distance computations.\n",
    "\n",
    "Speeds Up Convergence:\n",
    "\n",
    "Scaled features lead to faster and more stable distance calculations.\n",
    "\n",
    "Reduces computational complexity, especially for high-dimensional data.\n",
    "\n",
    "Prevents Skewed Predictions:\n",
    "\n",
    "Unscaled data can cause incorrect nearest neighbors selection.\n",
    "\n",
    "Leads to more reliable classifications and regressions.\n",
    "\n",
    "Common Scaling Techniques:\n",
    "Min-Max Scaling: Normalizes values between 0 and 1.\n",
    "\n",
    "Standardization (Z-score): Centers data around mean (0) with unit variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
